{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 4)\n"
     ]
    }
   ],
   "source": [
    "def get_queries() -> DataFrame:\n",
    "    # Read synonyms json-file\n",
    "    terms = pd.read_json('../data/raw/queries.json')\n",
    "\n",
    "    # Load pre-trained word vector model\n",
    "    word_vector_filepath = '../models/GoogleNews-vectors-negative300.bin'\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(\n",
    "        word_vector_filepath, binary=True, limit=400000\n",
    "    )\n",
    "\n",
    "    # Get similar word vectors\n",
    "    similar_terms = {}\n",
    "    for emotion in terms.columns:\n",
    "        similar_vectors = word_vectors.most_similar(emotion, topn=6)\n",
    "        search_terms = [term.replace('_',' ') for term, _ in similar_vectors]\n",
    "        similar_terms[emotion] = search_terms\n",
    "\n",
    "    return pd.concat([terms, DataFrame(similar_terms)]).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "df_query = get_queries()\n",
    "print(df_query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tweepy.api.API object at 0x00000298C77F4BE0>\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "filepath = '../api.key.json'\n",
    "keys = pd.read_json(filepath, typ='series')\n",
    "auth = tweepy.OAuth2BearerHandler(keys['BEARER_TOKEN'])\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "print(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18945, 4)\n"
     ]
    }
   ],
   "source": [
    "def create_df_by_queries(df_query:DataFrame, max_items:int) -> DataFrame:\n",
    "    \"\"\"\n",
    "        Iterates of list of search terms and uses each term as a search query.\n",
    "        Extracts tweet id, text, and timestamp and returns a Pandas DataFrame.\n",
    "        Params: max_items defines number of elements returned per search term.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for emotion in df_query.columns:\n",
    "        for search_term in df_query[emotion]:\n",
    "            if search_term.startswith(':'):\n",
    "                search_term = emoji.emojize(search_term)\n",
    "            search_term = f\"#{search_term.lower()} -filter:retweets\"\n",
    "            for status in tweepy.Cursor(api.search_tweets, search_term, count=100, lang='en').items(max_items):\n",
    "                json_str = status._json\n",
    "                id_ = json_str['id']\n",
    "                text = json_str['text']\n",
    "                timestamp = pd.to_datetime(json_str['created_at']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                dataset.append((id_, text, timestamp, emotion))\n",
    "    return DataFrame(dataset, columns=['Id','Text', 'CreatedAt', 'Label']).drop_duplicates(subset='Text', keep='first')\n",
    "\n",
    "df = create_df_by_queries(df_query, max_items=500)\n",
    "\n",
    "# Checkpoint\n",
    "df.to_csv('../data/raw/raw_emotions.csv', index=False)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy        6440\n",
       "fear       5039\n",
       "anger      3802\n",
       "sadness    3664\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick Analysis for better tags\n",
    "# from collections import Counter\n",
    "\n",
    "# df = pd.read_csv('../data/raw/raw_emotions.csv')\n",
    "\n",
    "# df_tmp = df[df['Label'] == 'anger']\n",
    "\n",
    "# hashtags = df_tmp['Text'].str.findall(r'#\\w+')\n",
    "\n",
    "# counter = Counter([tag for hashtag_list in hashtags.values for tag in hashtag_list])\n",
    "# counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## find emoticons\n",
    "# import re\n",
    "# import emoji\n",
    "# from collections import Counter\n",
    "# from warnings import filterwarnings\n",
    "# filterwarnings('ignore')\n",
    "\n",
    "# df = pd.read_csv('../data/raw/raw_emotions.csv')\n",
    "# df_tmp = df[df['Label'] == 'anger']\n",
    "\n",
    "# emoji_patterns = re.compile('|'.join(re.escape(p) for p in emoji.UNICODE_EMOJI_ENGLISH))\n",
    "\n",
    "# emoji_lists = df_tmp['Text'].str.findall(emoji_patterns)\n",
    "# emojis = [emoji for emoji_list in emoji_lists.values for emoji in emoji_list]\n",
    "\n",
    "# counter = Counter(emojis)\n",
    "# print(counter.most_common(n=10))\n",
    "\n",
    "# emoji_str = [emoji.demojize(e) for (e, _) in counter.most_common(n=10)]\n",
    "# print(emoji_str)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "557e8017941bd19fb6ffbcb9ac4042c9ef1a05ad323f01a83b04aa091443d0a7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp-emotion-recognition')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
